{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a81eb9a5",
   "metadata": {},
   "source": [
    "# üèóÔ∏è CE49X: AI Trends in Civil Engineering\n",
    "### üöÄ End-to-End Automated Data Analysis Pipeline\n",
    "\n",
    "**Project Overview:**\n",
    "This notebook integrates the entire research workflow, identifying the most prominent Artificial Intelligence technologies applied across various Civil Engineering sub-disciplines.\n",
    "\n",
    "** Workflow Architecture:**\n",
    "The pipeline consists of 4 main automated stages:\n",
    "\n",
    "1.  **üï∑Ô∏è Data Scraping:** Gathering raw data from RSS feeds, NewsAPI, and Google News.\n",
    "2.  **üß† NLP & Cleaning:** Content extraction (Selenium) and AI-powered summarization (Groq/Llama3).\n",
    "3.  **üè∑Ô∏è Classification:** Smart tagging using Dynamic Thresholds to categorize articles.\n",
    "4.  **üìä Visualization:** Generating insights via Network Graphs, Heatmaps, and Word Clouds.\n",
    "\n",
    "---\n",
    "### üõ†Ô∏è Step 1: Environment Setup\n",
    "The cell below imports all necessary Python libraries required for web scraping, data manipulation, Large Language Model (LLM) integration, and graphical plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f90973",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 1. LIBRARIES (FOR THE ENTIRE PROJECT)\n",
    "# ==========================================\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "import itertools\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "# Data Processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Natural Language Processing (NLP) & Machine Learning [UPDATED]\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Web Scraping & Browser Automation\n",
    "import requests\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from newspaper import Article\n",
    "\n",
    "# Artificial Intelligence (AI)\n",
    "from groq import Groq\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "# Enable inline plotting for Jupyter Notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# ==========================================\n",
    "# SYSTEM CONFIGURATION & DOWNLOADS\n",
    "# ==========================================\n",
    "\n",
    "# 1. Suppress Warnings (To keep the notebook output clean)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 2. NLTK Resource Check (Downloads automatically if missing)\n",
    "print(\"‚è≥ Checking NLTK resources...\")\n",
    "try:\n",
    "    nltk.data.find('tokenizers/punkt')\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    nltk.data.find('corpora/wordnet')\n",
    "    print(\"   - NLTK resources found.\")\n",
    "except LookupError:\n",
    "    print(\"   - Downloading missing NLTK resources...\")\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    print(\"   - Download complete.\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(\"‚úÖ ALL LIBRARIES INSTALLED AND LOADED SUCCESSFULLY!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daaf0136",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Step 2: Global Configurations & API Setup\n",
    "\n",
    "**‚ö†Ô∏è Action Required:** Please input your personal API keys in the variables below before proceeding.\n",
    "\n",
    "This cell acts as the **Central Control Unit** for the notebook. It defines:\n",
    "1.  **API Credentials:** Securely stores access keys for **NewsAPI** (Data Scraping) and **Groq** (LLM Processing).\n",
    "2.  **File Registry:** Standardizes input/output filenames to ensure seamless data flow between sequential tasks (e.g., Output of Task 1 -> Input of Task 2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4947e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. GLOBAL SETTINGS & API KEYS\n",
    "# ==========================================\n",
    "\n",
    "# üîë PASTE YOUR API KEYS HERE (Inside the quotes)\n",
    "# -----------------------------------------------------------\n",
    "NEWS_API_KEY = \"PASTE_YOUR_NEWS_API_KEY_HERE\" \n",
    "GROQ_API_KEY = \"PASTE_YOUR_GROQ_API_KEY_HERE\"\n",
    "GROQ_API_KEY_Ber = \"PASTE_YOUR_GROQ_API_KEY_HERE\"\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# üìÇ FILE NAMES (Constants for Automation)\n",
    "# Defining filenames here ensures consistency across all Tasks.\n",
    "FILE_RAW        = \"CE49X_RAW_DATA_1000_FULL.xlsx\"            # Task 1A Output (Scraping)\n",
    "FILE_WITH_TEXT  = \"CE49X_FINAL_DATASET_WITH_TEXT.xlsx\"       # Task 1B Output (Full Text)\n",
    "FILE_CLEAN_TEXT = \"CE49X_TASK1_FINAL_CLEAN.xlsx\"           # Task 1C Output (Pre-Cleaning)\n",
    "FILE_AI_SUMMARY = \"CE49X_TASK2_FINAL_SUBMISSIONBRT.xlsx\" # Task 2A Output (AI Summary & TF-IDF )\n",
    "FILE_NLP_FINAL  = \"CE49X_Final_Clean_Related_Articles.xlsx\"  # Task 2B Output (AI Filtered)\n",
    "FILE_NGRAM_REPORT = \"CE49X_Project_Top20Words_Top20bigrams_Top20Trigrams.xlsx\" # Task 2C Output (N-grams) \n",
    "FILE_TAGGED     = \"CE49X_Task3_Classification of Diciplines.xlsx\" # Task 3 Output (Tagged)\n",
    "\n",
    "\n",
    "# CONFIGURATION CHECK (Validates your input)\n",
    "print(f\"‚öôÔ∏è  Settings Loaded.\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# Check if the user is still using the placeholder \"PASTE\" or left it empty\n",
    "if \"PASTE\" in NEWS_API_KEY or len(NEWS_API_KEY) < 10:\n",
    "    print(f\"‚ùå WARNING: NewsAPI Key is missing or invalid!\")\n",
    "else:\n",
    "    print(f\"‚úÖ NewsAPI Key: Detected ({NEWS_API_KEY[:4]}***)\")\n",
    "\n",
    "if \"PASTE\" in GROQ_API_KEY or len(GROQ_API_KEY) < 10:\n",
    "    print(f\"‚ùå WARNING: Groq API Key is missing or invalid!\")\n",
    "else:\n",
    "    print(f\"‚úÖ Groq API Key: Detected ({GROQ_API_KEY[:4]}***)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2787e8f5",
   "metadata": {},
   "source": [
    "# üï∑Ô∏è Task 1A: Automated Data Scraping\n",
    "\n",
    "This section initiates the data pipeline by collecting raw information from the web.\n",
    "\n",
    "### üìã Data Overview\n",
    "We have successfully compiled a dataset containing **965 unique entries** related to Civil Engineering and Artificial Intelligence.\n",
    "\n",
    "| **Attribute** | **Description** |\n",
    "|:-------------|:----------------|\n",
    "| **Sources** | RSS Feeds, Google News, NewsAPI |\n",
    "| **Volume** | 965 Raw Articles |\n",
    "| **Condition** | Raw (Contains noise, duplicates, and non-relevant entries) |\n",
    "| **Output** | 'CE49X_RAW_DATA_1000_FULL.xlsx' |\n",
    "\n",
    "### üèóÔ∏è Data Schema\n",
    "The collected data is structured into the following columns:\n",
    "1.  **Title:** The main heading of the news or article.\n",
    "2.  **Date:** When the content was published.\n",
    "3.  **Source:** The publisher website.\n",
    "4.  **URL:** Link to the full text.\n",
    "5.  **Type:** The category of the source content.\n",
    "\n",
    "*‚ö†Ô∏è **System Check:** The code below checks for existing data files to prevent redundant scraping.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8cec8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# TASK 1A: DATA SCRAPING (Dependent on Global Settings)\n",
    "# ==========================================\n",
    "\n",
    "# SAFETY CHECK: Has Cell 2 (Settings) been run?\n",
    "if 'NEWS_API_KEY' not in globals() or 'FILE_RAW' not in globals():\n",
    "    print(\"‚ùå ERROR: Please run the top 'Cell 2' (Global Settings) first!\")\n",
    "else:\n",
    "    # Assign the global filename to a local variable for easier access\n",
    "    RAW_DATA_FILE = FILE_RAW \n",
    "\n",
    "    # 1. Check for Existing File\n",
    "    if os.path.exists(RAW_DATA_FILE):\n",
    "        print(f\"‚úÖ '{RAW_DATA_FILE}' already exists. Skipping scraping to save time/API limits.\")\n",
    "        df_raw = pd.read_excel(RAW_DATA_FILE)\n",
    "        \n",
    "        print(f\"\\nüìä CURRENT FILE STATUS:\")\n",
    "        print(f\"   -> Total Rows: {len(df_raw)}\")\n",
    "        display(df_raw.head()) \n",
    "\n",
    "    else:\n",
    "        print(\"üöÄ File not found, starting INTERNET SCRAPING process...\")\n",
    "        print(f\"üîë Using API Key: {NEWS_API_KEY[:5]}*** (Loaded from Cell 2)\")\n",
    "\n",
    "        # --- SEARCH KEYWORDS ---\n",
    "        civil_terms = [\"Construction\", \"Structural Engineering\", \"Geotechnical\", \"Transportation\", \"Infrastructure\", \"Concrete\", \"Bridge\", \"Tunnel\"]\n",
    "        ai_terms = [\"Artificial Intelligence\", \"Machine Learning\", \"Computer Vision\", \"Generative AI\", \"Neural Networks\", \"Robotics\", \"Automation\"]\n",
    "\n",
    "        # --- 1. RSS FUNCTION (Google News) ---\n",
    "        def fetch_rss_data():\n",
    "            print(f\"üì° Starting RSS Scraping...\")\n",
    "            articles = []\n",
    "            # Create all combinations (e.g., Bridge + Robotics)\n",
    "            combinations = list(itertools.product(civil_terms, ai_terms))\n",
    "            \n",
    "            # Google News RSS URL format (Targeting US/English)\n",
    "            base_url = \"https://news.google.com/rss/search?q={}+{}&hl=en-US&gl=US&ceid=US:en\"\n",
    "\n",
    "            for i, (civil, ai) in enumerate(combinations):\n",
    "                if len(articles) >= 1200: break \n",
    "                \n",
    "                query = f\"{civil} {ai} when:2y\"\n",
    "                final_url = base_url.format(query.replace(\" \", \"+\"), \"\")\n",
    "                \n",
    "                try:\n",
    "                    feed = feedparser.parse(final_url)\n",
    "                    for entry in feed.entries[:15]:\n",
    "                        articles.append({\n",
    "                            \"Title\": entry.title,\n",
    "                            \"Date\": entry.published if 'published' in entry else datetime.now().strftime(\"%Y-%m-%d\"),\n",
    "                            \"Source\": entry.source.title if 'source' in entry else \"Google News RSS\",\n",
    "                            \"URL\": entry.link,\n",
    "                            \"Type\": \"RSS (2-Year)\"\n",
    "                        })\n",
    "                except Exception as e:\n",
    "                    print(f\"Error: {e}\")\n",
    "                \n",
    "                # Polite delay to avoid IP ban\n",
    "                time.sleep(random.uniform(0.5, 0.8)) \n",
    "            \n",
    "            print(f\"   -> RSS Completed. Total fetched: {len(articles)}\")\n",
    "            return articles\n",
    "\n",
    "        # --- 2. API FUNCTION (NewsAPI) ---\n",
    "        def fetch_api_data(api_key):\n",
    "            # Check if the placeholder \"PASTE\" is still there or empty\n",
    "            if not api_key or \"PASTE\" in api_key:\n",
    "                print(\"‚ö†Ô∏è API Key is missing or invalid! Skipping API, using RSS data only.\")\n",
    "                return []\n",
    "                \n",
    "            print(f\"\\nüåç Starting API Scraping (NewsAPI)...\")\n",
    "            articles = []\n",
    "            queries = [\n",
    "                \"Civil Engineering Artificial Intelligence\",\n",
    "                \"Construction Robotics Automation\",\n",
    "                \"Structural Engineering Machine Learning\",\n",
    "                \"Concrete Computer Vision\",\n",
    "                \"Geotechnical Neural Networks\",\n",
    "                \"Transportation Generative AI\"\n",
    "            ]\n",
    "            url = \"https://newsapi.org/v2/everything\"\n",
    "            \n",
    "            for q in queries:\n",
    "                if len(articles) >= 400: break\n",
    "                \n",
    "                params = {\"q\": q, \"language\": \"en\", \"sortBy\": \"relevancy\", \"pageSize\": 100, \"apiKey\": api_key}\n",
    "                try:\n",
    "                    response = requests.get(url, params=params)\n",
    "                    data = response.json()\n",
    "                    if data.get(\"status\") == \"ok\":\n",
    "                        items = data.get(\"articles\", [])\n",
    "                        for item in items:\n",
    "                            articles.append({\n",
    "                                \"Title\": item[\"title\"],\n",
    "                                \"Date\": item[\"publishedAt\"],\n",
    "                                \"Source\": item[\"source\"][\"name\"],\n",
    "                                \"URL\": item[\"url\"],\n",
    "                                \"Type\": \"API (1-Month)\"\n",
    "                            })\n",
    "                        print(f\"   -> Query '{q}': {len(items)} articles found.\")\n",
    "                except Exception as e:\n",
    "                    print(f\"   -> API Connection Error: {e}\")\n",
    "            return articles\n",
    "\n",
    "        # --- EXECUTION ---\n",
    "        rss_data = fetch_rss_data()\n",
    "        \n",
    "        # Pass the global NEWS_API_KEY to the function\n",
    "        api_data = fetch_api_data(NEWS_API_KEY) \n",
    "\n",
    "        # MERGE AND CLEAN\n",
    "        all_data = rss_data + api_data\n",
    "        df_raw = pd.DataFrame(all_data)\n",
    "\n",
    "        print(\"\\nüîç Cleaning Data (Deduplication)...\")\n",
    "        initial_len = len(df_raw)\n",
    "        \n",
    "        # Remove duplicates based on URL\n",
    "        df_raw = df_raw.drop_duplicates(subset=['URL'])\n",
    "        \n",
    "        # Remove duplicates based on Title (Case insensitive)\n",
    "        df_raw['Title_Lower'] = df_raw['Title'].str.lower()\n",
    "        df_raw = df_raw.drop_duplicates(subset=['Title_Lower'])\n",
    "        df_raw = df_raw.drop(columns=['Title_Lower'])\n",
    "\n",
    "        print(f\"   -> {initial_len - len(df_raw)} duplicate records removed.\")\n",
    "        \n",
    "        # SAVE\n",
    "        df_raw.to_excel(RAW_DATA_FILE, index=False)\n",
    "        print(f\"‚úÖ PROCESS COMPLETE! File saved: {RAW_DATA_FILE}\")\n",
    "        print(f\"üìä Total Unique Articles: {len(df_raw)}\")\n",
    "        \n",
    "        display(df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed31882",
   "metadata": {},
   "source": [
    "# Task 1B Content Extraction & Full-Text Scraping\n",
    "\n",
    "## üéØ Objective\n",
    "Following the initial data collection, this module visits each unique URL to extract the **full textual content** of the articles. This step is crucial for transforming raw metadata (titles/links) into a rich corpus suitable for NLP analysis.\n",
    "\n",
    "## ‚öôÔ∏è Technical Approach\n",
    "* **Selenium WebDriver:** Handles dynamic content loading and redirects (e.g., bypassing intermediate Google News links).\n",
    "* **Newspaper3k Library:** Parses the HTML structure to isolate the main article text from ads, sidebars, and menus.\n",
    "* **Error Handling:** Implements a \"Force Restart\" mechanism to recover from browser crashes or connection timeouts automatically.\n",
    "\n",
    "## üìä Process\n",
    "1.  **Input:** `CE49X_RAW_DATA_1000_FULL.xlsx` (List of URLs).\n",
    "2.  **Action:** The script navigates to ~965 URLs, handles redirects, and scrapes the body text.\n",
    "3.  **Output:** `CE49X_FINAL_DATASET_WITH_TEXT.xlsx` (Dataset enriched with a `Full_Text` column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb78b33",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# TASK 1B: CONTENT EXTRACTION (Selenium & Newspaper3k)\n",
    "# ==========================================\n",
    "\n",
    "# CHECK: Are global settings loaded?\n",
    "if 'FILE_RAW' not in globals() or 'FILE_WITH_TEXT' not in globals():\n",
    "    print(\"‚ùå ERROR: Please run 'Cell 2' (Global Settings) first!\")\n",
    "    INPUT_FILE = \"CE49X_RAW_DATA_1000_FULL.xlsx\"       # Fallback\n",
    "    OUTPUT_FILE = \"CE49X_FINAL_DATASET_WITH_TEXT.xlsx\"  # Fallback\n",
    "else:\n",
    "    INPUT_FILE = FILE_RAW          # Input: Output of Task 1\n",
    "    OUTPUT_FILE = FILE_WITH_TEXT   # Output: This task's result\n",
    "\n",
    "# ==========================================\n",
    "# FILE EXISTENCE CHECK (LIKE TASK 1A)\n",
    "# ==========================================\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    print(f\"‚úÖ '{OUTPUT_FILE}' already exists. Skipping scraping to save time.\")\n",
    "    df_result = pd.read_excel(OUTPUT_FILE)\n",
    "    \n",
    "    print(f\"\\nüìä CURRENT FILE STATUS:\")\n",
    "    print(f\"   -> Total Rows: {len(df_result)}\")\n",
    "    print(f\"   -> Filled Texts: {df_result['Full_Text'].notna().sum()}\")\n",
    "    display(df_result.head())\n",
    "\n",
    "else:\n",
    "    print(f\"üöÄ Output file not found. Starting CONTENT EXTRACTION process...\")\n",
    "\n",
    "# ==========================================\n",
    "# BROWSER MANAGEMENT\n",
    "# ==========================================\n",
    "    def setup_driver():\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.page_load_strategy = 'eager'\n",
    "        \n",
    "        # Disable logs to prevent console clutter\n",
    "        chrome_options.add_argument(\"--log-level=3\") \n",
    "        \n",
    "        service = Service(ChromeDriverManager().install())\n",
    "        try:\n",
    "            driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "            driver.set_page_load_timeout(20)\n",
    "            return driver\n",
    "        except Exception as e:\n",
    "            print(f\"üö® Driver installation failed, retrying: {e}\")\n",
    "            time.sleep(5)\n",
    "            return webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "    def force_restart_driver(driver):\n",
    "        \"\"\"Forcibly closes and restarts the browser if it crashes.\"\"\"\n",
    "        print(\"\\n‚ôªÔ∏è  EMERGENCY: Restarting browser...\")\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass # Ignore errors if already closed\n",
    "        time.sleep(2)\n",
    "        return setup_driver()\n",
    "\n",
    "    # ==========================================\n",
    "    # CONTENT EXTRACTION LOGIC\n",
    "    # ==========================================\n",
    "    def get_full_content(driver, url):\n",
    "        try:\n",
    "            driver.get(url)\n",
    "            time.sleep(3)\n",
    "            \n",
    "            # Handle Google redirects\n",
    "            current_url = driver.current_url\n",
    "            if \"google.com\" in current_url:\n",
    "                time.sleep(2)\n",
    "                current_url = driver.current_url\n",
    "\n",
    "            article = Article(current_url)\n",
    "            article.download()\n",
    "            article.parse()\n",
    "            \n",
    "            if len(article.text) < 250:\n",
    "                return current_url, article.text, \"Short Content\"\n",
    "                \n",
    "            return current_url, article.text, \"Success\"\n",
    "\n",
    "        except Exception as e:\n",
    "            # Return error as string to handle in main loop\n",
    "            return url, \"\", str(e)\n",
    "\n",
    "    # ==========================================\n",
    "    # MAIN FLOW\n",
    "    # ==========================================\n",
    "    print(\"üìÇ Checking files...\")\n",
    "\n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        print(f\"‚úÖ Resuming from existing file: {OUTPUT_FILE}\")\n",
    "        df = pd.read_excel(OUTPUT_FILE)\n",
    "    else:\n",
    "        print(f\"üÜï Starting from scratch using: {INPUT_FILE}\")\n",
    "        if os.path.exists(INPUT_FILE):\n",
    "            df = pd.read_excel(INPUT_FILE)\n",
    "        else:\n",
    "            print(f\"‚ùå ERROR: Input file '{INPUT_FILE}' not found. Run Task 1 first.\")\n",
    "            df = pd.DataFrame() # Create empty to prevent crash\n",
    "\n",
    "    if not df.empty:\n",
    "        if 'Full_Text' not in df.columns:\n",
    "            df['Full_Text'] = \"\"\n",
    "            df['Final_URL'] = \"\"\n",
    "            df['Scrape_Status'] = \"\"\n",
    "\n",
    "        print(f\"üåê Starting Browser... (Scanning remaining rows)\")\n",
    "        driver = setup_driver()\n",
    "\n",
    "        save_interval = 5\n",
    "\n",
    "        for index, row in df.iterrows():\n",
    "            \n",
    "            # 1. Skip if already processed\n",
    "            if pd.notna(row['Full_Text']) and len(str(row['Full_Text'])) > 50:\n",
    "                continue\n",
    "\n",
    "            # 2. Periodic Cleanup (Every 50 steps)\n",
    "            if index > 0 and index % 50 == 0:\n",
    "                driver = force_restart_driver(driver)\n",
    "\n",
    "            # 3. Start Processing\n",
    "            original_url = row['URL']\n",
    "            print(f\"[{index+1}/{len(df)}] ‚è≥ Processing: {str(row['Title'])[:30]}...\")\n",
    "            \n",
    "            real_url, text, status = get_full_content(driver, original_url)\n",
    "            \n",
    "            # === CRITICAL UPDATE: CRASH CONTROL ===\n",
    "            # Check for connection errors or browser crashes\n",
    "            if \"HTTPConnectionPool\" in status or \"chrome not reachable\" in status.lower() or \"refused\" in status.lower():\n",
    "                print(f\"   ‚ùå Browser CRASHED! ({status[:30]}...)\")\n",
    "                \n",
    "                # Restart driver immediately\n",
    "                driver = force_restart_driver(driver)\n",
    "                \n",
    "                # Retry the SAME link\n",
    "                print(\"   üîÑ Retrying same link...\")\n",
    "                real_url, text, status = get_full_content(driver, original_url)\n",
    "            # ==========================================\n",
    "\n",
    "            # Save Results\n",
    "            df.at[index, 'Final_URL'] = real_url\n",
    "            df.at[index, 'Full_Text'] = text\n",
    "            df.at[index, 'Scrape_Status'] = status\n",
    "            \n",
    "            if status == \"Success\":\n",
    "                print(f\"   ‚úÖ OK | {len(str(text))} Chars\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è {status[:50]}\") \n",
    "                \n",
    "            if (index + 1) % save_interval == 0:\n",
    "                df.to_excel(OUTPUT_FILE, index=False)\n",
    "                print(f\"   üíæ Auto-saved.\")\n",
    "\n",
    "        # Cleanup\n",
    "        try:\n",
    "            driver.quit()\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        df.to_excel(OUTPUT_FILE, index=False)\n",
    "        print(\"\\nüèÅ PROCESS COMPLETED!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad7d5e8",
   "metadata": {},
   "source": [
    "# üßπ Task 1C: Data Sanitation & Pre-Filtering\n",
    "\n",
    "Before feeding the data into the NLP pipeline (Task 2), we must perform a **Quality Assurance (QA)** check. Sending broken or empty articles to the LLM (Large Language Model) would waste computational resources and API limits.\n",
    "\n",
    "### üîç Process Logic\n",
    "This script filters the dataset based on strict criteria:\n",
    "1.  **Status Check:** Retains only rows marked as `Success` during scraping.\n",
    "2.  **Length Validation:** Removes texts shorter than **200 characters** (often indicates captcha errors or empty pages).\n",
    "3.  **Deduplication:** Ensures every article is unique by checking `URL` and `Title`.\n",
    "\n",
    "* **Input:** Raw text data with potential errors.\n",
    "* **Output:** A clean, high-quality corpus ready for AI processing (`CE49X_TASK1_FINAL_CLEAN.xlsx`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51baf1e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# TASK 1C: DATA SANITATION (PRE-CLEANING)\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# CHECK: Are global settings loaded?\n",
    "if 'FILE_WITH_TEXT' not in globals() or 'FILE_CLEAN_TEXT' not in globals():\n",
    "    print(\"‚ùå ERROR: Please run 'Cell 2' (Global Settings) first!\")\n",
    "    # Fallback to defaults if Cell 2 wasn't run\n",
    "    INPUT_FILE = \"CE49X_FINAL_DATASET_WITH_TEXT.xlsx\"\n",
    "    OUTPUT_FILE = \"CE49X_TASK1_FINAL_CLEAN.xlsx\"\n",
    "else:\n",
    "    # Use global variables defined in Cell 2\n",
    "    INPUT_FILE = FILE_WITH_TEXT\n",
    "    OUTPUT_FILE = FILE_CLEAN_TEXT\n",
    "\n",
    "print(f\"üìÇ Loading dataset: {INPUT_FILE} ...\")\n",
    "\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(f\"‚ùå ERROR: File '{INPUT_FILE}' not found. Please run Task 1.5 first.\")\n",
    "else:\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "    initial_count = len(df)\n",
    "\n",
    "    # STEP 1: Filter out failed scrapes or empty text\n",
    "    # Keep only rows where Scrape_Status is 'Success' AND Full_Text is not empty\n",
    "    df_cleaned = df[\n",
    "        (df['Scrape_Status'] == 'Success') & \n",
    "        (df['Full_Text'].notna()) & \n",
    "        (df['Full_Text'].str.len() > 200) # Remove very short (likely error) texts\n",
    "    ].copy()\n",
    "\n",
    "    # STEP 2: Remove duplicate articles (Based on Title or URL)\n",
    "    # Project Requirement: \"Unique articles only\"\n",
    "    df_cleaned = df_cleaned.drop_duplicates(subset=['URL'])\n",
    "    df_cleaned = df_cleaned.drop_duplicates(subset=['Title'])\n",
    "\n",
    "    final_count = len(df_cleaned)\n",
    "\n",
    "    # STEP 3: Save the clean file\n",
    "    df_cleaned.to_excel(OUTPUT_FILE, index=False)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(f\"‚úÖ SANITATION COMPLETED!\")\n",
    "    print(f\"üì• Total Input Rows      : {initial_count}\")\n",
    "    print(f\"üì§ Successful Rows       : {final_count}\")\n",
    "    print(f\"üóëÔ∏è Removed (Trash/Fail)  : {initial_count - final_count}\")\n",
    "    print(f\"üíæ New File Saved As     : {OUTPUT_FILE}\")\n",
    "    print(\"=\"*40)\n",
    "\n",
    "    if final_count >= 500:\n",
    "        print(f\"üöÄ CONGRATULATIONS! You have {final_count} articles (Target: 500+ Passed).\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è WARNING: You have {final_count} articles remaining. You might need more data.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c20c49",
   "metadata": {},
   "source": [
    "# üèóÔ∏è Task 2A Civil Engineering News Analysis: NLP Pipeline & AI Summarization\n",
    "\n",
    "This notebook implements an automated Natural Language Processing (NLP) pipeline to analyze and summarize Civil Engineering news articles. The workflow integrates traditional NLP techniques with Generative AI to process raw text data into structured insights.\n",
    "\n",
    "### üöÄ Key Features & Methodology\n",
    "\n",
    "**1. Data Preprocessing (NLTK)**\n",
    "* **Cleaning:** Removal of punctuation, numbers, and short words.\n",
    "* **Normalization:** Lowercasing and tokenization.\n",
    "* **Filtering:** Removal of standard English stopwords and domain-specific noise words (e.g., \"subscribe\", \"advertisement\").\n",
    "* **Lemmatization:** Converting words to their base root form using `WordNetLemmatizer`.\n",
    "\n",
    "**2. Generative AI Summarization (Groq API)**\n",
    "* Utilizes the **Llama-3.1-8b** model via the Groq client.\n",
    "* **Prompt Engineering:** Generates concise summaries (max 23 words) that *must* explicitly classify the news into a specific construction field (e.g., Structural, Geotechnical).\n",
    "\n",
    "**3. Feature Extraction (TF-IDF)**\n",
    "* Calculates **Term Frequency-Inverse Document Frequency** scores to identify the most significant keywords for each individual article, highlighting unique topics.\n",
    "\n",
    "**5. Output**\n",
    "* The processed data, including AI summaries and TF-IDF keywords, is exported to `CE49X_TASK2_FINAL_SUBMISSIONBRT.xlsx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e017c8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 3. TASK 2A: AI SUMMARIZATION & TF-IDF\n",
    "# ==========================================\n",
    "\n",
    "# 1. SETTINGS & FILE NAMES\n",
    "# ------------------------------------------\n",
    "# Check if global settings are loaded from Cell 2\n",
    "if 'FILE_CLEAN_TEXT' not in globals() or 'FILE_AI_SUMMARY' not in globals():\n",
    "    print(\"‚ùå ERROR: Please run 'Cell 2' (Global Settings) first!\")\n",
    "    INPUT_FILE_PATH = \"CE49X_TASK1_FINAL_CLEAN.xlsx\"\n",
    "    OUTPUT_FILE_PATH = \"CE49X_TASK2_FINAL_SUBMISSIONBRT.xlsx\"\n",
    "else:\n",
    "    INPUT_FILE_PATH = FILE_CLEAN_TEXT  \n",
    "    OUTPUT_FILE_PATH = FILE_AI_SUMMARY \n",
    "\n",
    "# ==========================================\n",
    "# 2. FILE EXISTENCE CHECK (Save API Quota)\n",
    "# ==========================================\n",
    "if os.path.exists(OUTPUT_FILE_PATH):\n",
    "    # If the file exists, skip the process\n",
    "    print(f\"‚úÖ '{OUTPUT_FILE_PATH}' already exists. Skipping AI Analysis to save API quota.\")\n",
    "    print(\"   (Data is loaded and ready for next steps.)\")\n",
    "    df = pd.read_excel(OUTPUT_FILE_PATH)\n",
    "    display(df.head())\n",
    "\n",
    "else:\n",
    "    # If file is MISSING, start the process (All logic is inside this block)\n",
    "    print(\"üü¢ File not found. Starting AI Analysis & NLP Pipeline...\")\n",
    "\n",
    "    # API Key Check\n",
    "    if \"PASTE\" in GROQ_API_KEY_Ber or not GROQ_API_KEY_Ber:\n",
    "        print(\"‚ùå ERROR: Please define your GROQ_API_KEY_Ber in the Global Settings cell!\")\n",
    "    elif not os.path.exists(INPUT_FILE_PATH):\n",
    "        print(f\"‚ùå ERROR: Input file '{INPUT_FILE_PATH}' not found. Please run Task 1C first.\")\n",
    "    else:\n",
    "        # ------------------------------------------\n",
    "        # AI & NLP CONFIGURATION\n",
    "        # ------------------------------------------\n",
    "        # Using Groq API for processing\n",
    "        client = Groq(api_key=GROQ_API_KEY_Ber)\n",
    "        MODEL_NAME = \"llama-3.1-8b-instant\"\n",
    "\n",
    "        # NLP Setup\n",
    "        # NLTK lemmatizer and stop words configuration\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        custom_stop_words = set(stopwords.words('english'))\n",
    "        custom_stop_words.update(['subscribe', 'click', 'here', 'read', 'more', 'advertisement', 'copyright', 'share', 'civil', 'engineering']) \n",
    "\n",
    "        def clean_pipeline(text):\n",
    "            if not isinstance(text, str) or len(text) < 10: return \"\"\n",
    "            text = re.sub(r'[^\\w\\s]', '', text.lower())\n",
    "            text = re.sub(r'\\d+', '', text)\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            cleaned = [lemmatizer.lemmatize(w) for w in tokens if w not in custom_stop_words and len(w) > 2]\n",
    "            return \" \".join(cleaned)\n",
    "\n",
    "        def get_summary_groq(text):\n",
    "            try:\n",
    "                time.sleep(1.2) # Rate limit precaution\n",
    "                prompt = (\n",
    "                    f\"Summarize this Civil Engineering news in 1 concise sentence (max 23 words). \"\n",
    "                    f\"You MUST explicitly mention which specific construction field \"\n",
    "                    f\"(e.g., Structural, Geotechnical, Materials, Transport) is involved: {text[:2500]}\"\n",
    "                )\n",
    "                chat_completion = client.chat.completions.create(\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    model=MODEL_NAME, temperature=0.5,\n",
    "                )\n",
    "                return chat_completion.choices[0].message.content.strip()\n",
    "            except Exception as e:\n",
    "                if \"429\" in str(e):\n",
    "                    time.sleep(10) # Wait if rate limited\n",
    "                    return get_summary_groq(text)\n",
    "                return \"Summary Error\"\n",
    "\n",
    "        # Read Data & Process\n",
    "        try:\n",
    "            if INPUT_FILE_PATH.endswith('.csv'): df = pd.read_csv(INPUT_FILE_PATH)\n",
    "            else: df = pd.read_excel(INPUT_FILE_PATH)\n",
    "            \n",
    "            summaries = []\n",
    "            cleaned_texts = []\n",
    "            print(f\"üöÄ Processing {len(df)} articles...\")\n",
    "\n",
    "            for index, row in df.iterrows():\n",
    "                if (index + 1) % 10 == 0: print(f\"[{index+1}/{len(df)}] Processing...\")\n",
    "                raw = str(row['Full_Text'])\n",
    "                summaries.append(get_summary_groq(raw))\n",
    "                cleaned_texts.append(clean_pipeline(raw))\n",
    "\n",
    "            df['AI_Summary'] = summaries\n",
    "            df['Cleaned_Text_NLP'] = cleaned_texts\n",
    "\n",
    "            # TF-IDF Calculation\n",
    "            print(\"\\nüìä Calculating TF-IDF Scores...\")\n",
    "            vectorizer = TfidfVectorizer(max_features=1000)\n",
    "            tfidf_matrix = vectorizer.fit_transform(df['Cleaned_Text_NLP'].fillna(\"\"))\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            top_keywords = []\n",
    "            for i in range(tfidf_matrix.shape[0]):\n",
    "                row = tfidf_matrix[i]\n",
    "                if row.nnz > 0:\n",
    "                    top_indices = row.toarray()[0].argsort()[-5:][::-1]\n",
    "                    keywords = [feature_names[idx] for idx in top_indices if row[0, idx] > 0]\n",
    "                    top_keywords.append(\", \".join(keywords))\n",
    "                else: top_keywords.append(\"\")\n",
    "            df['Top_TFIDF_Keywords'] = top_keywords\n",
    "\n",
    "            # Save to File\n",
    "            df.to_excel(OUTPUT_FILE_PATH, index=False)\n",
    "            print(f\"‚úÖ NEW ANALYSIS SAVED: {OUTPUT_FILE_PATH}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2840c622",
   "metadata": {},
   "source": [
    "# üõ°Ô∏è Task 2B: AI Relevance Filtering \n",
    "\n",
    "**NOTE:** This step uses the dataset (`CE49X_TASK2_FINAL_SUBMISSIONBRT.xlsx`).\n",
    "\n",
    "## üéØ Objective\n",
    "To perform a semantic quality check on the provided dataset using the **Groq API (Llama 3)**. The AI acts as a Civil Engineering Professor to filter out irrelevant content.\n",
    "\n",
    "## ‚öôÔ∏è Process\n",
    "1.  **Input:** External File defined in Cell 2 (`FILE_AI_SUMMARY`).\n",
    "2.  **Action:** The AI reads the Title/Summary and decides: **Keep** or **Reject**.\n",
    "3.  **Output:** Saves to the standard pipeline filename (`CE49X_Final_Clean_Related_Articles.xlsx`) so Task 3 can read it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f25cc59",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# TASK 2B: AI RELEVANCE FILTERING (EXTERNAL INPUT)\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "from groq import Groq\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETTINGS & SETUP\n",
    "# ==========================================\n",
    "\n",
    "# CHECK: Are global settings (Cell 2) loaded?\n",
    "if 'GROQ_API_KEY' not in globals() or 'FILE_EXTERNAL_INPUT' not in globals():\n",
    "    print(\"‚ùå ERROR: Please run 'Cell 2' (Global Settings) first!\")\n",
    "    # Fallback variables just in case\n",
    "    INPUT_FILE = \"CE49X_TASK2_FINAL_SUBMISSIONBRT.xlsx\" \n",
    "    FINAL_FILE = \"CE49X_Final_Clean_Related_Articles.xlsx\"\n",
    "    CURRENT_API_KEY = \"\" \n",
    "else:\n",
    "    # Use Global Variables\n",
    "    INPUT_FILE = FILE_AI_SUMMARY\n",
    "    FINAL_FILE = FILE_NLP_FINAL\n",
    "    CURRENT_API_KEY = GROQ_API_KEY_Ber\n",
    "\n",
    "# Temporary Files\n",
    "TEMP_FILE = \"TEMP_ALL_DATA_BACKUP.xlsx\"\n",
    "LOG_FILE = \"completed_indices.txt\"\n",
    "\n",
    "# Initialize Client\n",
    "if not CURRENT_API_KEY or \"PASTE\" in CURRENT_API_KEY:\n",
    "    print(\"‚ùå ERROR: Groq API Key is missing! Check Cell 2.\")\n",
    "    client = None\n",
    "else:\n",
    "    client = Groq(api_key=CURRENT_API_KEY)\n",
    "\n",
    "# ==========================================\n",
    "# 2. FILE LOADING\n",
    "# ==========================================\n",
    "print(f\"üìÇ Reading file: {INPUT_FILE}\")\n",
    "\n",
    "def load_file(path):\n",
    "    try: return pd.read_csv(path)\n",
    "    except:\n",
    "        try: return pd.read_csv(path, sep=';')\n",
    "        except: return pd.read_excel(path)\n",
    "\n",
    "if os.path.exists(FINAL_FILE):\n",
    "    print(f\"‚úÖ '{FINAL_FILE}' already exists. Skipping AI Filtering.\")\n",
    "    df = pd.read_excel(FINAL_FILE) # Load existing for display if needed\n",
    "    print(f\"üíé Clean Data Count: {len(df)}\")\n",
    "else:\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"‚ùå ERROR: Input file '{INPUT_FILE}' not found!\")\n",
    "        df = pd.DataFrame()\n",
    "    else:\n",
    "        try:\n",
    "            df = load_file(INPUT_FILE)\n",
    "            print(f\"‚úÖ File loaded. Initial Row Count: {len(df)}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå ERROR: Could not read file. ({e})\")\n",
    "            df = pd.DataFrame()\n",
    "\n",
    "# Proceed only if client exists and data is loaded (and output doesn't exist)\n",
    "if client and not df.empty and not os.path.exists(FINAL_FILE):\n",
    "\n",
    "    # Column Mapping\n",
    "    text_col = 'AI_Summary' if 'AI_Summary' in df.columns else 'Full_Text'\n",
    "    # Fallback if specific columns don't exist\n",
    "    if text_col not in df.columns: text_col = df.columns[1]\n",
    "\n",
    "    title_col = 'Title' if 'Title' in df.columns else df.columns[0]\n",
    "\n",
    "    # Initialize Status Columns\n",
    "    if 'is_civil_ai' not in df.columns: df['is_civil_ai'] = False\n",
    "    if 'Status_Detail' not in df.columns: df['Status_Detail'] = \"Pending\"\n",
    "\n",
    "    print(f\"‚ÑπÔ∏è  Using columns -> Title: '{title_col}', Text: '{text_col}'\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. SAFE API FUNCTION\n",
    "    # ==========================================\n",
    "    def ask_groq_safe(prompt):\n",
    "        wait_time = 20\n",
    "        while True:\n",
    "            try:\n",
    "                completion = client.chat.completions.create(\n",
    "                    model=\"llama-3.3-70b-versatile\",\n",
    "                    messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "                    temperature=0,\n",
    "                    max_tokens=150,\n",
    "                )\n",
    "                return completion.choices[0].message.content.strip()\n",
    "            except Exception as e:\n",
    "                error_msg = str(e).lower()\n",
    "                if \"429\" in error_msg or \"rate limit\" in error_msg:\n",
    "                    print(f\"\\n‚ö†Ô∏è RATE LIMIT (429). Waiting {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    wait_time += 10\n",
    "                else:\n",
    "                    print(f\"\\n‚ùå Error: {e}. Retrying in 5s.\")\n",
    "                    time.sleep(5)\n",
    "\n",
    "    # ==========================================\n",
    "    # 4. RESUME CAPABILITY\n",
    "    # ==========================================\n",
    "    completed_indices = set()\n",
    "    if os.path.exists(LOG_FILE):\n",
    "        with open(LOG_FILE, 'r') as f:\n",
    "            for line in f:\n",
    "                try: completed_indices.add(int(line.strip()))\n",
    "                except: pass\n",
    "\n",
    "    todo_indices = [i for i in df.index if i not in completed_indices]\n",
    "    BATCH_SIZE = 10 \n",
    "    total_steps = (len(todo_indices) // BATCH_SIZE) + 1\n",
    "\n",
    "    print(f\"‚è≠Ô∏è  Already Completed: {len(completed_indices)}\")\n",
    "    print(f\"üöÄ  Remaining to Process: {len(todo_indices)}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # ==========================================\n",
    "    # 5. MAIN LOOP\n",
    "    # ==========================================\n",
    "    step = 0\n",
    "    for i in range(0, len(todo_indices), BATCH_SIZE):\n",
    "        step += 1\n",
    "        batch_indices = todo_indices[i : i+BATCH_SIZE]\n",
    "        batch_df = df.loc[batch_indices]\n",
    "        \n",
    "        batch_text = \"\"\n",
    "        for idx, row in batch_df.iterrows():\n",
    "            summary = str(row[text_col])[:700].replace(\"\\n\", \" \") \n",
    "            title = str(row[title_col])[:150]\n",
    "            batch_text += f\"Paper_ID {idx}: [TITLE: {title}] [SUMMARY: {summary}]\\n\\n\"\n",
    "\n",
    "        # --- PROMPT ---\n",
    "        prompt = f\"\"\"\n",
    "        Act as a Civil Engineering Professor.\n",
    "        Filter papers to keep ONLY those relevant to Civil Engineering, Construction, and the Built Environment.\n",
    "        \n",
    "        CRITERIA FOR INCLUSION (YES - KEEP):\n",
    "        1. Core Civil: Structures, Geotechnics, Transport, Materials, Hydraulics.\n",
    "        2. Construction: Management, Safety, BIM, Digital Twins, Heavy Equipment.\n",
    "        3. Related Built Environment: Architecture, Urban Planning, Smart Cities.\n",
    "        4. AI Applications: ANY AI/ML paper applied to the domains above.\n",
    "        \n",
    "        CRITERIA FOR EXCLUSION (NO - REJECT):\n",
    "        1. Pure Non-Civil Fields: Medical, Finance, Pure Biology, Agriculture.\n",
    "        2. General CS: Pure algorithms, NLP, or Gaming without a construction use case.\n",
    "        \n",
    "        DECISION RULE:\n",
    "        If a paper is borderline, INCLUDE IT.\n",
    "        \n",
    "        INPUT LIST:\n",
    "        {batch_text}\n",
    "        \n",
    "        OUTPUT FORMAT:\n",
    "        Return ONLY a JSON list of ACCEPTED Paper_ID integers. Example: [102, 105]\n",
    "        If none are relevant, return [].\n",
    "        \"\"\"\n",
    "        \n",
    "        print(f\"\\nüîÑ Processing Batch {step}/{total_steps}...\", end=\"\")\n",
    "        response = ask_groq_safe(prompt)\n",
    "        \n",
    "        accepted_ids = []\n",
    "        try:\n",
    "            match = re.search(r'\\[.*?\\]', response)\n",
    "            if match:\n",
    "                accepted_ids = json.loads(match.group(0))\n",
    "        except: pass\n",
    "\n",
    "        print(f\" (‚úÖ {len(accepted_ids)} Accepted)\")\n",
    "\n",
    "        for idx in batch_indices:\n",
    "            title_short = str(df.loc[idx, title_col])[:40] + \"...\"\n",
    "            if idx in accepted_ids:\n",
    "                print(f\"  üü¢ [KEEP] ID {idx}: {title_short}\")\n",
    "                df.loc[idx, 'is_civil_ai'] = True\n",
    "                df.loc[idx, 'Status_Detail'] = \"ACCEPTED\"\n",
    "            else:\n",
    "                print(f\"  üî¥ [DROP] ID {idx}: {title_short}\")\n",
    "                df.loc[idx, 'is_civil_ai'] = False\n",
    "                df.loc[idx, 'Status_Detail'] = \"REJECTED\"\n",
    "\n",
    "        # Save Progress\n",
    "        with open(LOG_FILE, 'a') as f:\n",
    "            for x in batch_indices:\n",
    "                f.write(f\"{x}\\n\")\n",
    "        \n",
    "        try: df.to_excel(TEMP_FILE, index=False)\n",
    "        except: pass\n",
    "            \n",
    "        time.sleep(1)\n",
    "\n",
    "    # ==========================================\n",
    "    # 6. FINAL CLEANUP\n",
    "    # ==========================================\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üßπ FINALIZING: Removing rejected articles...\")\n",
    "\n",
    "    df_clean = df[df['is_civil_ai'] == True].copy()\n",
    "    cols_to_drop = ['is_civil_ai', 'Status_Detail']\n",
    "    df_clean.drop(columns=[c for c in cols_to_drop if c in df_clean.columns], inplace=True)\n",
    "\n",
    "    df_clean.to_excel(FINAL_FILE, index=False)\n",
    "\n",
    "    print(f\"‚úÖ PROCESS COMPLETED!\")\n",
    "    print(f\"üìÑ Original Data Count: {len(df)}\")\n",
    "    print(f\"üíé Final Clean Data Count: {len(df_clean)}\")\n",
    "    print(f\"üíæ FILE READY: {FINAL_FILE}\")\n",
    "    print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc95ddb2",
   "metadata": {},
   "source": [
    "# üìä Task 2C: Advanced Keyword Analysis (N-Grams)\n",
    "\n",
    "## üéØ Objective\n",
    "To extract the most frequent and meaningful terminology from the dataset, we perform an **N-Gram Analysis**. This goes beyond simple word counting by identifying common phrases and technical terms.\n",
    "\n",
    "## üßπ Noise Filtering Strategy\n",
    "To ensure the results are relevant to Engineering, we apply a **Dual-Layer Stop Word Filter**:\n",
    "1.  **Hard Filter (Unigrams):** Removes generic terms like \"system,\" \"model,\" \"data,\" and \"analysis\" to reveal specific topics (e.g., \"Concrete,\" \"Seismic\").\n",
    "2.  **Soft Filter (Bi/Trigrams):** Allows generic terms only when they form technical phrases (e.g., \"Machine Learning,\" \"Data Analysis,\" \"Structural Health Monitoring\").\n",
    "\n",
    "## üìà Output\n",
    "* **Unigrams:** Top single keywords (Topic indicators).\n",
    "* **Bigrams:** Top 2-word phrases (Technology pairs).\n",
    "* **Trigrams:** Top 3-word phrases (Specific methodologies).\n",
    "* **Excel Report:** `CE49X_Project_Top20Words_Top20bigrams_Top20Trigrams.xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7529b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# TASK 2C: ADVANCED KEYWORD ANALYSIS (N-GRAMS)\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "import os\n",
    "\n",
    "# 1. CHECK GLOBAL SETTINGS\n",
    "# ---------------------------------------------------------\n",
    "if 'FILE_NLP_FINAL' not in globals() or 'FILE_NGRAM_REPORT' not in globals():\n",
    "    print(\"‚ùå ERROR: Please run 'Cell 2' (Global Settings) first!\")\n",
    "    # Fallback defaults\n",
    "    INPUT_FILE = \"CE49X_Final_Clean_Related_Articles.xlsx\"\n",
    "    OUTPUT_FILE = \"CE49X_Project_Top20Words_Top20bigrams_Top20Trigrams.xlsx\"\n",
    "else:\n",
    "    # Use variables from Cell 2\n",
    "    INPUT_FILE = FILE_NLP_FINAL      # Input: Clean Data from Task 2A\n",
    "    OUTPUT_FILE = FILE_NGRAM_REPORT  # Output: The report file\n",
    "\n",
    "print(f\"üìä Reading dataset: {INPUT_FILE}\")\n",
    "\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(f\"‚ùå ERROR: File '{INPUT_FILE}' not found! Run Task 2A first.\")\n",
    "else:\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. TEXT MERGING (FULL SCAN)\n",
    "    # ==========================================\n",
    "    # We combine available text columns for maximum context\n",
    "    col1 = 'Cleaned_Text_NLP' if 'Cleaned_Text_NLP' in df.columns else ''\n",
    "    col2 = 'AI_Summary' if 'AI_Summary' in df.columns else ''\n",
    "    \n",
    "    # Fallback to Full_Text or Title if columns are missing\n",
    "    if not col1 and not col2:\n",
    "        col1 = 'Full_Text' if 'Full_Text' in df.columns else 'Title'\n",
    "\n",
    "    print(f\"üîç Analyzing Columns: {col1} + {col2}\")\n",
    "\n",
    "    df['Combined_Text'] = (df[col1].fillna(\"\").astype(str) + \" \" + df[col2].fillna(\"\").astype(str)).str.lower()\n",
    "    texts = df['Combined_Text'].tolist()\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. STOP WORDS CONFIGURATION\n",
    "    # ==========================================\n",
    "    # Common junk words to exclude from all lists\n",
    "    common_junk = [\n",
    "        'google', 'scholar', 'crossref', 'text', 'full', 'view', 'download', 'pdf',\n",
    "        'citation', 'cited', 'copyright', 'author', 'rights', 'reserved', 'license',\n",
    "        'journal', 'publishing', 'publisher', 'volume', 'issue', 'peer', 'review',\n",
    "        'online', 'library', 'access', 'open', 'website', 'page', 'web', 'homepage',\n",
    "        'prediction', 'error', 'accuracy', 'mean', 'absolute', 'training', 'testing', \n",
    "        'validation', 'train', 'test', 'predict', 'predicted', 'values', 'value',\n",
    "        'dataset', 'data', 'point', 'points', 'set', 'sets', 'center', 'centers',\n",
    "        'proposed', 'approach', 'method', 'methodology', 'result', 'results',\n",
    "        'performance', 'comparison', 'compared', 'experimental', 'study', 'studies',\n",
    "        'using', 'used', 'use', 'based', 'table', 'figure', 'fig', 'doi', 'vol', 'no', \n",
    "        'pp', 'al', 'et', 'url', 'http', 'https', 'click', 'size', \n",
    "        'image', 'shown', 'presented', 'article', 'paper', 'work',\n",
    "        'usd', 'billion', 'million', 'market', 'growth', 'report', 'forecast',\n",
    "        'correlation', 'coefficient', 'input', 'feature', 'hidden', 'layer',\n",
    "        'north', 'america', 'new', 'like', 'good', 'better', 'best', 'high', 'low', \n",
    "        'different', 'real', 'case', 'making', 'potential', 'need', 'needs', \n",
    "        'company', 'companies', 'parameter', 'parameters', 'information', 'technology',\n",
    "        'significant', 'significantly', 'increase', 'decreased', 'impact'\n",
    "    ]\n",
    "\n",
    "    # Words banned only in Unigrams (Single words)\n",
    "    single_word_bans = [\n",
    "        'system', 'systems', 'model', 'models', 'analysis', 'algorithm', 'algorithms',\n",
    "        'application', 'applications', 'process', 'project', 'design', 'development',\n",
    "        'research', 'time', 'year', 'number', 'level', 'quality', 'state',\n",
    "        'machine', 'learning', 'artificial', 'intelligence', 'deep', 'neural', 'network', 'networks', \n",
    "        'make', 'said', 'including', 'problem', 'solution', 'engineering', 'civil'\n",
    "    ]\n",
    "\n",
    "    stops_soft = list(ENGLISH_STOP_WORDS) + common_junk        \n",
    "    stops_hard = stops_soft + single_word_bans                 \n",
    "\n",
    "    # ==========================================\n",
    "    # 4. ANALYSIS ENGINE\n",
    "    # ==========================================\n",
    "    def get_clean_top_n(corpus, n=20, n_gram=(1,1), custom_stops=None):\n",
    "        if not corpus: return []\n",
    "        try:\n",
    "            vec = CountVectorizer(stop_words=custom_stops, ngram_range=n_gram, max_features=10000).fit(corpus)\n",
    "            bag_of_words = vec.transform(corpus)\n",
    "            sum_words = bag_of_words.sum(axis=0) \n",
    "            words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "            sorted_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "            \n",
    "            clean_list = []\n",
    "            for word, freq in sorted_freq:\n",
    "                if \" \" in word:\n",
    "                    parts = word.split()\n",
    "                    if len(set(parts)) != len(parts): continue \n",
    "                clean_list.append((word, freq))\n",
    "                if len(clean_list) == n: break\n",
    "            return clean_list\n",
    "        except ValueError:\n",
    "            return []\n",
    "\n",
    "    print(\"‚è≥ Running detailed N-Gram analysis...\")\n",
    "\n",
    "    # 1. UNIGRAMS\n",
    "    top_words = get_clean_top_n(texts, 20, (1, 1), custom_stops=stops_hard)\n",
    "    # 2. BIGRAMS\n",
    "    top_bigrams = get_clean_top_n(texts, 20, (2, 2), custom_stops=stops_soft)\n",
    "    # 3. TRIGRAMS\n",
    "    top_trigrams = get_clean_top_n(texts, 20, (3, 3), custom_stops=stops_soft)\n",
    "\n",
    "    # ==========================================\n",
    "    # 5. DISPLAY AND SAVE\n",
    "    # ==========================================\n",
    "    def print_table(title, data):\n",
    "        print(\"\\n\" + \"=\"*50)\n",
    "        print(f\"üèÜ {title}\")\n",
    "        print(\"=\"*50)\n",
    "        for w, f in data:\n",
    "            print(f\"{w}: {f}\")\n",
    "\n",
    "    print_table(\"TOP 20 KEYWORDS (Unigrams)\", top_words)\n",
    "    print_table(\"TOP 20 PAIRS (Bigrams)\", top_bigrams)\n",
    "    print_table(\"TOP 20 TRIPLETS (Trigrams)\", top_trigrams)\n",
    "\n",
    "    # Save to Excel\n",
    "    print(f\"\\nüíæ Saving Report to: {OUTPUT_FILE}\")\n",
    "    with pd.ExcelWriter(OUTPUT_FILE, engine='openpyxl') as writer:\n",
    "        pd.DataFrame(top_words, columns=['Word', 'Frequency']).to_excel(writer, sheet_name='Top20_Words', index=False)\n",
    "        pd.DataFrame(top_bigrams, columns=['BiGram', 'Frequency']).to_excel(writer, sheet_name='Top20_Bigrams', index=False)\n",
    "        pd.DataFrame(top_trigrams, columns=['TriGram', 'Frequency']).to_excel(writer, sheet_name='Top20_Trigrams', index=False)\n",
    "\n",
    "    print(f\"‚úÖ ANALYSIS REPORT READY!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70ef05e",
   "metadata": {},
   "source": [
    "# üè∑Ô∏è Task 3: Intelligent Classification & Visualization\n",
    "\n",
    "## üéØ Objective\n",
    "This module categorizes the filtered articles into specific **Civil Engineering sub-domains** (e.g., *Structural, Geotechnical*) and **AI Technologies** (e.g., *Computer Vision, Predictive Analytics*).\n",
    "\n",
    "## üß† Dynamic Thresholding Logic\n",
    "To ensure high accuracy, we apply different keyword density thresholds based on our data analysis:\n",
    "* **High Threshold (4):** Applied to broad categories like *Construction Management* and *Robotics* to reduce noise.\n",
    "* **Low Threshold (2):** Applied to niche categories like *Geotechnical* and *Generative Design* to capture rare but relevant papers.\n",
    "\n",
    "## üìä Visual Outputs\n",
    "1.  **Heatmap:** Visualizes the cross-disciplinary density (e.g., How often is *Computer Vision* used in *Structural Engineering*?).\n",
    "2.  **Trend Graph:** Tracks the popularity of top AI+Civil combinations over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cca92dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# TASK 3: CLASSIFICATION & VISUALIZATION\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# CHECK: Are global settings loaded?\n",
    "if 'FILE_NLP_FINAL' not in globals() or 'FILE_TAGGED' not in globals():\n",
    "    print(\"‚ùå ERROR: Please run 'Cell 2' (Global Settings) first!\")\n",
    "    INPUT_FILE = \"CE49X_Final_Clean_Related_Articles.xlsx\"\n",
    "    OUTPUT_FILE = \"CE49X_Task3_Classification of Diciplines.xlsx\"\n",
    "else:\n",
    "    INPUT_FILE = FILE_NLP_FINAL   # Input: Output of Task 2A\n",
    "    OUTPUT_FILE = FILE_TAGGED     # Output: This task's result\n",
    "\n",
    "# IMAGE OUTPUTS\n",
    "HEATMAP_FILE = \"CE49X_Task3_Heatmap.png\"\n",
    "TREND_FILE = \"CE49X_Task3_Trend_Analysis_COMBINATIONS.png\"\n",
    "DATE_COL = \"Date\"\n",
    "\n",
    "print(f\"üìÇ Reading File: {INPUT_FILE}\")\n",
    "\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(f\"‚ùå ERROR: File '{INPUT_FILE}' not found. Run Task 2.5 first.\")\n",
    "else:\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "\n",
    "    # ==========================================\n",
    "    # 1. TEXT PREPARATION\n",
    "    # ==========================================\n",
    "    # Combine relevant text columns for searching\n",
    "    col1 = 'Cleaned_Text_NLP' if 'Cleaned_Text_NLP' in df.columns else ''\n",
    "    col2 = 'AI_Summary' if 'AI_Summary' in df.columns else ''\n",
    "    \n",
    "    # Fallback if columns don't exist\n",
    "    if not col1 and not col2: col1 = 'Full_Text'\n",
    "\n",
    "    df['search_text'] = (df[col1].fillna(\"\").astype(str) + \" \" + df[col2].fillna(\"\").astype(str)).str.lower()\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. KEYWORDS DICTIONARY\n",
    "    # ==========================================\n",
    "    ce_keywords = {\n",
    "        \"Structural\": [\"structural\", \"structure\", \"beam\", \"column\", \"concrete\", \"steel\", \"bridge\", \"seismic\", \"earthquake\", \"health monitoring\", \"shm\", \"crack\", \"damage\", \"masonry\", \"reinforced\", \"compressive\", \"tensile\"],\n",
    "        \"Geotechnical\": [\"geotechnical\", \"soil\", \"rock\", \"foundation\", \"tunnel\", \"excavation\", \"slope\", \"stability\", \"landslide\", \"underground\", \"pile\", \"earth\", \"clay\", \"sand\", \"liquefaction\"],\n",
    "        \"Transportation\": [\"transport\", \"transportation\", \"traffic\", \"road\", \"highway\", \"vehicle\", \"autonomous\", \"driverless\", \"logistics\", \"pavement\", \"asphalt\", \"flow\", \"pedestrian\", \"congestion\"],\n",
    "        \"Construction Mgmt\": [\"management\", \"scheduling\", \"schedule\", \"cost\", \"estimation\", \"safety\", \"site\", \"worker\", \"risk\", \"bim\", \"building information\", \"planning\", \"contract\", \"supply chain\"],\n",
    "        \"Environmental\": [\"environmental\", \"sustainability\", \"sustainable\", \"waste\", \"green\", \"energy\", \"carbon\", \"emission\", \"water\", \"pollution\", \"climate\", \"lifecycle\", \"lca\"]\n",
    "    }\n",
    "\n",
    "    ai_keywords = {\n",
    "        \"Computer Vision\": [\"vision\", \"image\", \"camera\", \"video\", \"detection\", \"recognition\", \"cnn\", \"convolutional\", \"object detection\", \"segmentation\", \"drone\", \"uav\", \"surveillance\"],\n",
    "        \"Predictive Analytics\": [\"prediction\", \"predictive\", \"forecast\", \"forecasting\", \"regression\", \"classification\", \"machine learning\", \"deep learning\", \"neural network\", \"ann\", \"lstm\", \"random forest\", \"svm\", \"risk assessment\", \"decision tree\"],\n",
    "        \"Generative Design\": [\"generative\", \"optimization\", \"genetic algorithm\", \"evolutionary\", \"parametric\", \"topology\", \"design optimization\"],\n",
    "        \"Robotics/Automation\": [\"robot\", \"robotics\", \"automation\", \"automated\", \"autonomous\", \"3d printing\", \"additive manufacturing\", \"sensor\", \"iot\", \"internet of things\"]\n",
    "    }\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. DYNAMIC THRESHOLD LOGIC üß†\n",
    "    # ==========================================\n",
    "    print(\"üè∑Ô∏è  Starting Classification (Using Optimized Thresholds)...\")\n",
    "\n",
    "    # --- CIVIL ENG THRESHOLDS ---\n",
    "    CE_THRESHOLDS = {\n",
    "        \"Structural\": 4,        \n",
    "        \"Transportation\": 4,    \n",
    "        \"Construction Mgmt\": 4, \n",
    "        \"Geotechnical\": 2,      \n",
    "        \"Environmental\": 2      \n",
    "    }\n",
    "\n",
    "    # --- AI THRESHOLDS ---\n",
    "    AI_THRESHOLDS = {\n",
    "        \"Robotics/Automation\": 4,  # High noise -> High threshold\n",
    "        \"Predictive Analytics\": 4, # High noise -> High threshold\n",
    "        \"Computer Vision\": 3,      # Average -> Medium threshold\n",
    "        \"Generative Design\": 2     # Niche -> Low threshold\n",
    "    }\n",
    "\n",
    "    def get_flexible_tags(text, keyword_dict, threshold_dict=None, default_threshold=2):\n",
    "        scores = {category: 0 for category in keyword_dict}\n",
    "        \n",
    "        for category, words in keyword_dict.items():\n",
    "            for word in words:\n",
    "                scores[category] += text.count(word)\n",
    "        \n",
    "        winners = []\n",
    "        \n",
    "        for cat, score in scores.items():\n",
    "            limit = threshold_dict.get(cat, default_threshold) if threshold_dict else default_threshold\n",
    "            if score >= limit:\n",
    "                winners.append(cat)\n",
    "                \n",
    "        # Recovery Mode: If no category passes the threshold, take the max score\n",
    "        if not winners and max(scores.values()) > 0:\n",
    "            max_val = max(scores.values())\n",
    "            winners = [cat for cat, score in scores.items() if score == max_val]\n",
    "            \n",
    "        return winners if winners else [\"Other\"]\n",
    "\n",
    "    # Apply Tagging\n",
    "    df['CE_Area'] = df['search_text'].apply(lambda x: get_flexible_tags(x, ce_keywords, threshold_dict=CE_THRESHOLDS))\n",
    "    df['AI_Tech'] = df['search_text'].apply(lambda x: get_flexible_tags(x, ai_keywords, threshold_dict=AI_THRESHOLDS))\n",
    "\n",
    "    # Save to Excel\n",
    "    df['CE_Area_Str'] = df['CE_Area'].apply(lambda x: \", \".join(x))\n",
    "    df['AI_Tech_Str'] = df['AI_Tech'].apply(lambda x: \", \".join(x))\n",
    "\n",
    "    url_col = 'Final_URL' if 'Final_URL' in df.columns else 'URL'\n",
    "    # Rename Final_URL to URL for cleaner output\n",
    "    df.rename(columns={url_col: 'URL'}, inplace=True)\n",
    "    \n",
    "    output_cols = ['Title', DATE_COL, 'CE_Area_Str', 'AI_Tech_Str', 'URL', 'search_text']\n",
    "    df_save = df[[c for c in output_cols if c in df.columns]].copy()\n",
    "\n",
    "    df_save.to_excel(OUTPUT_FILE, index=False)\n",
    "    print(f\"‚úÖ Classifications saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 4. HEATMAP VISUALIZATION\n",
    "    # ==========================================\n",
    "    matrix_data = pd.DataFrame(0, index=ce_keywords.keys(), columns=ai_keywords.keys())\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        ce_tags = row['CE_Area']\n",
    "        ai_tags = row['AI_Tech']\n",
    "        if \"Other\" in ce_tags or \"Other\" in ai_tags: continue\n",
    "        for ce in ce_tags:\n",
    "            for ai in ai_tags:\n",
    "                matrix_data.loc[ce, ai] += 1\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.heatmap(matrix_data, annot=True, fmt=\"d\", cmap=\"Blues\", linewidths=.5)\n",
    "    plt.title(\"Civil Engineering Areas vs. AI Technologies\", fontsize=12)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(HEATMAP_FILE, dpi=300)\n",
    "    print(f\"üñºÔ∏è  Heatmap saved: {HEATMAP_FILE}\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 5. TREND ANALYSIS (TIME SERIES)\n",
    "    # ==========================================\n",
    "    print(f\"\\nüìà Generating Trend Analysis...\")\n",
    "    if DATE_COL in df.columns:\n",
    "        try:\n",
    "            # Convert Date\n",
    "            df['Date_Object'] = pd.to_datetime(df[DATE_COL], errors='coerce', utc=True)\n",
    "            df = df.dropna(subset=['Date_Object'])\n",
    "            df['Month_Str'] = df['Date_Object'].dt.strftime('%Y-%m')\n",
    "            \n",
    "            # Explode lists to handle multiple tags\n",
    "            df_exploded = df.explode('CE_Area').explode('AI_Tech')\n",
    "            df_exploded = df_exploded[(df_exploded['CE_Area'] != 'Other') & (df_exploded['AI_Tech'] != 'Other')]\n",
    "            \n",
    "            # Create Combinations\n",
    "            df_exploded['Combination'] = df_exploded['CE_Area'] + \" + \" + df_exploded['AI_Tech']\n",
    "            \n",
    "            # Find Top 5 Combinations\n",
    "            top_combos = df_exploded['Combination'].value_counts().head(5).index.tolist()\n",
    "            df_final_trend = df_exploded[df_exploded['Combination'].isin(top_combos)]\n",
    "            \n",
    "            # Pivot Data for Plotting\n",
    "            trend_data = df_final_trend.groupby(['Month_Str', 'Combination']).size().unstack(fill_value=0)\n",
    "            \n",
    "            # Plot\n",
    "            plt.figure(figsize=(14, 7))\n",
    "            sns.lineplot(data=trend_data, marker=\"o\", linewidth=2.5)\n",
    "            plt.title(\"Top AI + Civil Engineering Research Trends Over Time\", fontsize=14)\n",
    "            plt.ylabel(\"Article Count\")\n",
    "            plt.xlabel(\"Month\")\n",
    "            plt.legend(title=\"Research Areas\", bbox_to_anchor=(1.01, 1), loc='upper left')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.grid(True, linestyle='--', alpha=0.7)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(TREND_FILE, dpi=300)\n",
    "            print(f\"üìà Trend Graph saved: {TREND_FILE}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Trend Analysis Error: {e}\")\n",
    "\n",
    "    print(\"\\n‚úÖ TASK 3 COMPLETED SUCCESSFULLY!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e39188c",
   "metadata": {},
   "source": [
    "# üìä Task 4: Visualization Generation (Final Outputs)\n",
    "\n",
    "## üéØ Objective\n",
    "This module transforms the analyzed data into publication-ready visual assets to demonstrate the relationships between Civil Engineering domains and AI technologies.\n",
    "\n",
    "## üñºÔ∏è Generated Visuals\n",
    "1.  **Bar Chart:** Displays the distribution of articles across the 5 main Civil Engineering disciplines using dynamic thresholds.\n",
    "2.  **Network Graph:** Maps the co-occurrence strength between specific engineering terms (e.g., *Seismic*) and AI methods (e.g., *Neural Networks*). Thicker lines indicate stronger connections.\n",
    "3.  **Word Clouds:** Generates specific word clouds for each discipline, filtering out generic academic noise to highlight niche terminology.\n",
    "\n",
    "## üìÇ Output\n",
    "All images are automatically saved to the folder: `Task4_Final_Outputs_Synchronized/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf8f1e2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# TASK 4: VISUALIZATION GENERATION\n",
    "# ==========================================\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import os\n",
    "from collections import Counter\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "# CHECK: Are global settings loaded?\n",
    "if 'FILE_NLP_FINAL' not in globals():\n",
    "    print(\"‚ùå ERROR: Please run 'Cell 2' (Global Settings) first!\")\n",
    "    INPUT_FILE = \"CE49X_Final_Clean_Related_Articles.xlsx\"\n",
    "else:\n",
    "    INPUT_FILE = FILE_NLP_FINAL\n",
    "\n",
    "# üî• AUTOMATIC FOLDER CREATION\n",
    "base_dir = os.path.dirname(os.path.abspath(INPUT_FILE)) if os.path.exists(INPUT_FILE) else \".\"\n",
    "OUTPUT_FOLDER = os.path.join(base_dir, \"Task4_Final_Outputs_Synchronized\")\n",
    "\n",
    "if not os.path.exists(OUTPUT_FOLDER):\n",
    "    os.makedirs(OUTPUT_FOLDER)\n",
    "\n",
    "print(f\"üìÇ Reading File: {INPUT_FILE}\")\n",
    "if not os.path.exists(INPUT_FILE):\n",
    "    print(\"‚ùå ERROR: File not found! Run Task 2A first.\")\n",
    "else:\n",
    "    df = pd.read_excel(INPUT_FILE)\n",
    "\n",
    "    # Text Merging\n",
    "    col1 = 'Cleaned_Text_NLP' if 'Cleaned_Text_NLP' in df.columns else ''\n",
    "    col2 = 'AI_Summary' if 'AI_Summary' in df.columns else ''\n",
    "    # Fallback to Full_Text if columns missing\n",
    "    if not col1 and not col2: col1 = 'Full_Text'\n",
    "    \n",
    "    df['Combined_Text'] = (df[col1].fillna(\"\").astype(str) + \" \" + df[col2].fillna(\"\").astype(str)).str.lower()\n",
    "\n",
    "    # ==========================================\n",
    "    # 2. KEYWORDS DICTIONARY\n",
    "    # ==========================================\n",
    "    ce_keywords = {\n",
    "        \"Structural\": [\"structural\", \"structure\", \"beam\", \"column\", \"concrete\", \"steel\", \"bridge\", \"seismic\", \"earthquake\", \"health monitoring\", \"shm\", \"crack\", \"damage\", \"masonry\", \"reinforced\", \"compressive\", \"tensile\"],\n",
    "        \"Geotechnical\": [\"geotechnical\", \"soil\", \"rock\", \"foundation\", \"tunnel\", \"excavation\", \"slope\", \"stability\", \"landslide\", \"underground\", \"pile\", \"earth\", \"clay\", \"sand\", \"liquefaction\"],\n",
    "        \"Transportation\": [\"transport\", \"transportation\", \"traffic\", \"road\", \"highway\", \"vehicle\", \"autonomous\", \"driverless\", \"logistics\", \"pavement\", \"asphalt\", \"flow\", \"pedestrian\", \"congestion\"],\n",
    "        \"Construction Mgmt\": [\"management\", \"scheduling\", \"schedule\", \"cost\", \"estimation\", \"safety\", \"site\", \"worker\", \"risk\", \"bim\", \"building information\", \"planning\", \"contract\", \"supply chain\"],\n",
    "        \"Environmental\": [\"environmental\", \"sustainability\", \"sustainable\", \"waste\", \"green\", \"energy\", \"carbon\", \"emission\", \"water\", \"pollution\", \"climate\", \"lifecycle\", \"lca\"]\n",
    "    }\n",
    "\n",
    "    ai_keywords = {\n",
    "        \"Computer Vision\": [\"vision\", \"image\", \"camera\", \"video\", \"detection\", \"recognition\", \"cnn\", \"convolutional\", \"object detection\", \"segmentation\", \"drone\", \"uav\", \"surveillance\"],\n",
    "        \"Predictive Analytics\": [\"prediction\", \"predictive\", \"forecast\", \"forecasting\", \"regression\", \"classification\", \"machine learning\", \"deep learning\", \"neural network\", \"ann\", \"lstm\", \"random forest\", \"svm\", \"risk assessment\", \"decision tree\"],\n",
    "        \"Generative Design\": [\"generative\", \"optimization\", \"genetic algorithm\", \"evolutionary\", \"parametric\", \"topology\", \"design optimization\"],\n",
    "        \"Robotics/Automation\": [\"robot\", \"robotics\", \"automation\", \"automated\", \"autonomous\", \"3d printing\", \"additive manufacturing\", \"sensor\", \"iot\", \"internet of things\"]\n",
    "    }\n",
    "\n",
    "    # ==========================================\n",
    "    # 3. TAGGING FOR BAR CHART (DYNAMIC THRESHOLDS üß†)\n",
    "    # ==========================================\n",
    "    print(\"üè∑Ô∏è  Preparing Bar Chart Data (Dynamic Thresholds)...\")\n",
    "\n",
    "    DYNAMIC_THRESHOLDS = {\n",
    "        \"Structural\": 4,        \n",
    "        \"Transportation\": 4,    \n",
    "        \"Construction Mgmt\": 4, \n",
    "        \"Geotechnical\": 2,      \n",
    "        \"Environmental\": 2      \n",
    "    }\n",
    "\n",
    "    def get_flexible_tags(text, keyword_dict, threshold_dict=None, default_threshold=2):\n",
    "        scores = {category: 0 for category in keyword_dict}\n",
    "        for category, words in keyword_dict.items():\n",
    "            for word in words:\n",
    "                scores[category] += text.count(word)\n",
    "        \n",
    "        winners = []\n",
    "        for cat, score in scores.items():\n",
    "            limit = threshold_dict.get(cat, default_threshold) if threshold_dict else default_threshold\n",
    "            if score >= limit:\n",
    "                winners.append(cat)\n",
    "                \n",
    "        # Recovery mechanism\n",
    "        if not winners and max(scores.values()) > 0:\n",
    "            max_val = max(scores.values())\n",
    "            winners = [cat for cat, score in scores.items() if score == max_val]\n",
    "            \n",
    "        return winners\n",
    "\n",
    "    # Multi-label tagging for Bar Chart\n",
    "    df['CE_Area_Bar'] = df['Combined_Text'].apply(lambda x: get_flexible_tags(x, ce_keywords, threshold_dict=DYNAMIC_THRESHOLDS))\n",
    "\n",
    "    # ==========================================\n",
    "    # 4. VISUAL 1: BAR CHART\n",
    "    # ==========================================\n",
    "    print(\"üìä 1. Generating Bar Chart...\")\n",
    "    df_exploded = df.explode('CE_Area_Bar')\n",
    "    df_exploded = df_exploded[df_exploded['CE_Area_Bar'].notna()] \n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    ax = sns.countplot(y=\"CE_Area_Bar\", data=df_exploded, \n",
    "                       order=df_exploded['CE_Area_Bar'].value_counts().index, \n",
    "                       palette=\"viridis\")\n",
    "    plt.title(\"Number of Articles per Civil Engineering Area (Dynamic Thresholds)\", fontsize=14)\n",
    "    plt.xlabel(\"Article Count\")\n",
    "    plt.ylabel(\"CE Sub-Discipline\")\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_FOLDER}/1_BarChart_Count_SYNC.png\", dpi=300)\n",
    "    print(\"   ‚úÖ Bar Chart saved.\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 5. VISUAL 2: NETWORK GRAPH (PURPLE THEME) üíú\n",
    "    # ==========================================\n",
    "    print(\"üï∏Ô∏è  2. Generating Network Graph...\")\n",
    "\n",
    "    ce_flat = [w for words in ce_keywords.values() for w in words]\n",
    "    ai_flat = [w for words in ai_keywords.values() for w in words]\n",
    "    all_specific_terms = ce_flat + ai_flat\n",
    "\n",
    "    term_pairs = []\n",
    "    for text in df['Combined_Text']:\n",
    "        found_terms = [term for term in all_specific_terms if term in text]\n",
    "        if len(found_terms) > 1:\n",
    "            possible_pairs = list(itertools.combinations(set(found_terms), 2))\n",
    "            for t1, t2 in possible_pairs:\n",
    "                # Only connect CE terms to AI terms (Cross-domain)\n",
    "                is_cross = (t1 in ce_flat and t2 in ai_flat) or (t1 in ai_flat and t2 in ce_flat)\n",
    "                if is_cross:\n",
    "                    term_pairs.append((t1, t2))\n",
    "\n",
    "    pair_counts = Counter(term_pairs)\n",
    "    filtered_pairs = {k: v for k, v in pair_counts.items() if v >= 2}\n",
    "    top_pairs = sorted(filtered_pairs.items(), key=lambda x: x[1], reverse=True)[:35]\n",
    "\n",
    "    if top_pairs:\n",
    "        G = nx.Graph()\n",
    "        for (term1, term2), weight in top_pairs:\n",
    "            G.add_edge(term1, term2, weight=weight)\n",
    "\n",
    "        plt.figure(figsize=(15, 12))\n",
    "        pos = nx.spring_layout(G, k=2.5, seed=42)\n",
    "        \n",
    "        # Node Colors\n",
    "        node_colors = ['#D6EAF8' if node in ce_flat else '#FADBD8' for node in G.nodes()]\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, node_size=2800, edgecolors='#512E5F', linewidths=1.0)\n",
    "\n",
    "        # Edges\n",
    "        edges = G.edges(data=True)\n",
    "        weights = [d['weight'] for u, v, d in edges]\n",
    "        w_sorted = sorted(weights)\n",
    "        p33 = np.percentile(w_sorted, 33)\n",
    "        p66 = np.percentile(w_sorted, 66)\n",
    "\n",
    "        tier_weak = []\n",
    "        tier_medium = []\n",
    "        tier_strong = []\n",
    "\n",
    "        for u, v, d in edges:\n",
    "            w = d['weight']\n",
    "            if w >= p66: tier_strong.append((u, v))\n",
    "            elif w >= p33: tier_medium.append((u, v))\n",
    "            else: tier_weak.append((u, v))\n",
    "\n",
    "        # Draw Edges (Purple Scale)\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=tier_weak, width=3.0, alpha=0.75, edge_color='#C39BD3')\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=tier_medium, width=5.0, alpha=0.85, edge_color='#8E44AD')\n",
    "        nx.draw_networkx_edges(G, pos, edgelist=tier_strong, width=8.0, alpha=1.0, edge_color='#4A235A')\n",
    "\n",
    "        nx.draw_networkx_labels(G, pos, font_size=11, font_weight='bold', bbox=dict(facecolor='white', alpha=0.85, edgecolor='none', pad=1))\n",
    "        plt.title(\"Civil Engineering & AI: Connectivity Strength (Purple Scale)\", fontsize=18)\n",
    "        plt.axis('off')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{OUTPUT_FOLDER}/2_Network_Graph_PURPLE_FINAL.png\", dpi=300)\n",
    "        print(\"   ‚úÖ Network Graph saved.\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 6. VISUAL 3: WORD CLOUDS (WINNER TAKES ALL) ‚òÅÔ∏è\n",
    "    # ==========================================\n",
    "    print(\"‚òÅÔ∏è  3. Generating Word Clouds (Winner Takes All + Clean)...\")\n",
    "\n",
    "    # Function to find dominant category\n",
    "    def get_dominant_category(text):\n",
    "        scores = {area: 0 for area in ce_keywords.keys()}\n",
    "        words_in_text = set(text.lower().split())\n",
    "        for area, keywords in ce_keywords.items():\n",
    "            match_count = len(words_in_text.intersection(keywords))\n",
    "            scores[area] = match_count\n",
    "        best_area = max(scores, key=scores.get)\n",
    "        if scores[best_area] == 0: return None \n",
    "        return best_area\n",
    "\n",
    "    df['Dominant_Area'] = df['Combined_Text'].apply(get_dominant_category)\n",
    "\n",
    "    # Aggressive Stopwords List\n",
    "    custom_stops = set(STOPWORDS)\n",
    "    junk_words = [\n",
    "        'fig', 'figs', 'figure', 'figures', 'data', 'dataset', 'database', 'model', 'modeling', 'models',\n",
    "        'use', 'using', 'used', 'study', 'paper', 'method', 'result', 'analysis', 'analyze', 'performance',\n",
    "        'system', 'application', 'approach', 'prediction', 'accuracy', 'time', 'cost', 'optimization',\n",
    "        'development', 'based', 'proposed', 'new', 'value', 'image', 'including', 'shown', 'said', 'number',\n",
    "        'design', 'construction', 'project', 'infrastructure', 'one', 'within', 'usd', 'set', 'make', 'solution',\n",
    "        'monitoring', 'control', 'technology', 'tool', 'information', 'process', 'feature', 'parameter', \n",
    "        'show', 'provide', 'case', 'example', 'good', 'better', 'high', 'low', 'large', 'small', 'different',\n",
    "        'real-time', 'efficiency', 'challenge', 'function', 'table', 'eq', 'civil', 'engineering'\n",
    "    ]\n",
    "    custom_stops.update(junk_words)\n",
    "\n",
    "    for area in ce_keywords.keys():\n",
    "        subset = df[df['Dominant_Area'] == area]\n",
    "        if len(subset) > 5:\n",
    "            text_pool = \" \".join(subset['Combined_Text'].tolist())\n",
    "            wc = WordCloud(width=900, height=500, background_color='white', stopwords=custom_stops, \n",
    "                           max_words=40, colormap='magma', collocations=False, random_state=42).generate(text_pool)\n",
    "            \n",
    "            plt.figure(figsize=(12, 6))\n",
    "            plt.imshow(wc, interpolation='bilinear')\n",
    "            plt.axis(\"off\")\n",
    "            plt.title(f\"Word Cloud: {area} (Exclusive Articles)\", fontsize=16, fontweight='bold', color='#4A235A')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{OUTPUT_FOLDER}/3_WordCloud_{area}_FINAL.png\", dpi=300)\n",
    "            plt.close()\n",
    "            print(f\"   -> Word Cloud generated: {area}\")\n",
    "        else:\n",
    "            print(f\"      ‚ö†Ô∏è Not enough data for {area}.\")\n",
    "\n",
    "    print(\"\\n‚úÖ ALL VISUALIZATIONS COMPLETED!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
